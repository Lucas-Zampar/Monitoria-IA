{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba6795a",
   "metadata": {},
   "source": [
    "# Introdução "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4a58ba",
   "metadata": {},
   "source": [
    "Neste notebook, faremos algumas visualizações do dataset cora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a862976",
   "metadata": {},
   "source": [
    "# 1- Dataset no formato tabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39ff3e5",
   "metadata": {},
   "source": [
    "A representação em grafos do dataset cora também está disponível no formato csv através da página do grupo <a href='https://linqs.soe.ucsc.edu/data'> LINQS </a>. No entanto, não é necessário realizar o download, pois ele já consta na pasta <a href='cora.csv'>cora.csv</a> do diretório atual deste notebook. Para tanto, vamos referenciar o arquivo <code>cora.content</code> (matriz de características e classes) e <code>cora.cites</code> (relações de citação)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff74458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_content = 'cora.csv/cora.content'\n",
    "path_cites = 'cora.csv/cora.cites'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b790b2a4",
   "metadata": {},
   "source": [
    "Em seguida, vamos importar as tabela a partir do <code>pandas</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674e1969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "#Carrega citações\n",
    "cites = pd.read_csv(path_cites, sep='\\t', header=None, names=['target','source'])\n",
    "\n",
    "#Nome das colunas referentes às palavras\n",
    "feature_names = [f'P_{_}' for _ in range(1433)]\n",
    "\n",
    "#Define o nome de todas as colunas para a tabela de características e classificações\n",
    "column_names = ['id'] + feature_names + ['label']\n",
    "\n",
    "#Carrega características e classificações\n",
    "content = pd.read_csv(path_content, sep='\\t', header=None, names=column_names)\n",
    "\n",
    "#Ordena a tabela de acordo com o id do artigo\n",
    "content = content.sort_values('id')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e614be52",
   "metadata": {},
   "source": [
    "Agora, vamos visualizar a tabela contendo as características e classificações:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365dd2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac9e070",
   "metadata": {},
   "source": [
    "Agora, vamos visualizar a tabela contendo as citações. Nesse caso, vamos transpô-la para melhorar a visualização:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbb3b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "cites.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ed5de",
   "metadata": {},
   "source": [
    "# 2- Análise de PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f7834e",
   "metadata": {},
   "source": [
    "Segundo o <a fref= 'Introduction to the theory of computation_third edition - Michael Sipser.pdf'> artigo </a> que referencia o dataset cora que empregamos até o momento, durante o preprocessamento das publicações do dataset, houve a remoção de _stopwords_ e normalização textual. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0b56d8",
   "metadata": {},
   "source": [
    ">__stopwords__: palavras mais comuns em uma língua. Por exemplo, no inglês, poderíamos citar: “the”, “is”, “in”, “for”, “where”, “when”, “to”, “at” etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b34c532",
   "metadata": {},
   "source": [
    "Para tanto, vamos carregar um pdf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021c5f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge pdfplumber -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44feeb43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "#Carrega o PDF\n",
    "pdf = pdfplumber.open('a.pdf')\n",
    "#Verifica a quantidade de páginas\n",
    "num_pages = len(pdf.pages)\n",
    "\n",
    "#String para o texto do PDF\n",
    "text_from_pdf = ''\n",
    "#Para cada página, extrai o texto \n",
    "for _ in range(num_pages):\n",
    "    text_from_pdf +=  pdf.pages[_].extract_text()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5887f6",
   "metadata": {},
   "source": [
    "Primeiro, precisamos realizar a tokenização do texto. Vamos fazer isso com o módulo spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ad989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge spacy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8d63be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f54867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "example_sent = \"\"\"This is a sample sentence,\n",
    "                  showing off the stop words filtration.\"\"\"\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "word_tokens = word_tokenize(example_sent)\n",
    " \n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    " \n",
    "filtered_sentence = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9888d1",
   "metadata": {},
   "source": [
    "Agora, vamos remover as _stopwords_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a6fcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3640d6c",
   "metadata": {},
   "source": [
    "Como é possível pereceber, há muitos tokens como pontuações, dígitos e caracteres de escape. Vamos retirá-los:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9108b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "#Removendo pontuações\n",
    "filtered_sentence = [_ for _ in filtered_sentence if _ not in string.punctuation]\n",
    "#Removendo dígitos \n",
    "filtered_sentence = [_ for _ in filtered_sentence if _ not in string.digits]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae797e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2532982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.Series(filtered_sentence).value_counts().to_frame()[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa57fc67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adfe4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d157deaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a12bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8728e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9591b583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da582960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804863c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85156e11",
   "metadata": {},
   "source": [
    "Inicializando o dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7789f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root='.', name='Cora')\n",
    "\n",
    "num_node_features = dataset.num_node_features\n",
    "embedding_size = 16\n",
    "num_classes = dataset.num_classes\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bfd4bf",
   "metadata": {},
   "source": [
    "Implementando o modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d9dd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, ChebConv, GATConv, SGConv\n",
    "\n",
    "#x = F.dropout(x, training=self.training) #Dropout\n",
    "\n",
    "class GCN(Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SGConv(in_channels, 16, 2)\n",
    "        self.conv2 = SGConv(16, out_channels, 2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index) \n",
    "        x = F.relu(x) \n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index) \n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "'''\n",
    "#O modelo herda a classe Module (base para qualquer modelo de nn)\n",
    "class GCN(Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 16)\n",
    "        self.conv2 = GCNConv(16, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \n",
    "        x = self.conv1(x, edge_index) \n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x, training=self.training) #Dropout\n",
    "        x = self.conv2(x, edge_index) \n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8e89c1",
   "metadata": {},
   "source": [
    "Escolha do dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8decf09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48908e0f",
   "metadata": {},
   "source": [
    "Transferindo modelo e dataset para o dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a724b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(num_node_features, num_classes).to(device)\n",
    "data = dataset[0].to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f09bc0c",
   "metadata": {},
   "source": [
    "Escolhendo a função de custo e o otimizador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867dfdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4) \n",
    "\n",
    "#loss_function = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4) \n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=5e-4) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5a2b2c",
   "metadata": {},
   "source": [
    "Definindo a função de treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c34095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    optimizer.zero_grad() #Limpa o gradiente\n",
    "    out = model(data.x, data.edge_index) # Realiza a propagação\n",
    "    loss = loss_function(out[data.train_mask], data.y[data.train_mask]) # Computa o loss apenas do conjunto de treinamento\n",
    "    loss.backward()  # Calcula o gradiente \n",
    "    optimizer.step() # Atualiza os parâmetros do modelo com base no gradiente\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde7c23e",
   "metadata": {},
   "source": [
    "Definindo a função de avaliação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804f4807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aval_train():\n",
    "    pred = model(data.x, data.edge_index).argmax(dim=1) # Realiza predições\n",
    "    correct = (pred[data.val_mask] == data.y[data.val_mask]).sum() # Soma os acertos totais\n",
    "    acc = int(correct) / int(data.val_mask.sum()) # Calcula a acurácia acertos / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3fd2fe",
   "metadata": {},
   "source": [
    "Treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1402f974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "accuracies = []\n",
    "losses = []\n",
    "\n",
    "epochs = np.arange(1,num_epochs+1)\n",
    "\n",
    "for epoch in epochs:\n",
    "    loss = train() #Realiza o treinamento\n",
    "    accuracy = aval_train() #Calcula a acuráciapNLLLoss()\n",
    "    losses.append(float(loss)) #Anexa o loss atual a lista de losses\n",
    "    accuracies.append(accuracy) # Anexa a acurácia atual na lista de acuracias\n",
    "    if epoch%50 == 0: print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Acurácia: {accuracy}') #Imprima o status a cada 50 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885b20c9",
   "metadata": {},
   "source": [
    "Plotando os resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8c006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib notebook\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "ax.plot(epochs, losses, color='red')\n",
    "ax.plot(epochs, accuracies, color='blue')\n",
    "ax.set(xlabel='EPOCH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf1ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
